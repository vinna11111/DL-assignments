Deep Learning Assignments (IIT Hyderabad â€“ AI2100/AI5100/EE6380)

This repository contains my solutions to the Deep Learning course assignments at IIT Hyderabad (Spring 2025).
Each assignment is implemented in a separate Jupyter Notebook (.ipynb) and covers key deep learning concepts â€” from foundational algorithms to modern architectures like Transformers and VAEs.

â¸»

ğŸ“‚ Repository Structure
	â€¢	Assignment 1 â€“ Perceptron, Gradient Descent, and MLP
	â€¢	Assignment 2 â€“ 3D CNNs & Sequence Models (Elman, LSTM, GRU)
	â€¢	Assignment 3 â€“ Word2Vec & Vision Transformer (ViT)
	â€¢	Assignment 4 â€“ Autoencoders & Variational Autoencoders (VAE)

â¸»

ğŸ“˜ Assignment Details

Assignment 1 â€“ Fundamentals of Deep Learning
	â€¢	Implemented Perceptron Learning Algorithm on linearly separable and noisy datasets.
	â€¢	Implemented Gradient Descent for binary classification, comparing with Perceptron.
	â€¢	Built an MLP (Multi-layer Perceptron) with a single hidden layer from scratch, including:
	â€¢	Forward pass
	â€¢	Backpropagation
	â€¢	Gradient updates
	â€¢	Experiments with weight initialization, learning rates, loss functions, and hidden layer size.

ğŸ““ Notebook: Assignment1.ipynb

â¸»

Assignment 2 â€“ Convolutional Neural Networks & Sequence Models
	â€¢	Implemented a 3D Convolutional Neural Network (3D CNN) for MNIST point-cloud data:
	â€¢	3D Convolution
	â€¢	3D Pooling (Max, Avg, Global Avg)
	â€¢	MLP classifier
	â€¢	Complete 3D CNN pipeline with classification
	â€¢	Designed a dataset for Balanced Parentheses Counting and trained sequence models:
	â€¢	Elman RNN
	â€¢	LSTM
	â€¢	GRU
	â€¢	Compared learning curves and performance against a simple baseline.

ğŸ““ Notebook: Assignment2.ipynb

â¸»

Assignment 3 â€“ Word2Vec & Vision Transformer
	â€¢	Word2Vec (Skip-gram with Negative Sampling):
	â€¢	Preprocessed text8 dataset (tokenization, vocabulary creation).
	â€¢	Implemented Skip-gram with negative sampling from scratch.
	â€¢	Trained embeddings and evaluated via:
	â€¢	Visualization (SVD projections)
	â€¢	Word similarity tasks (e.g., analogies like king - man + woman â‰ˆ queen).
	â€¢	Analyzed impact of hyperparameters.
	â€¢	Vision Transformer (ViT) on CIFAR-10:
	â€¢	Implemented an Encoder-only ViT: patch embeddings, positional encodings, Transformer encoder, CLS token.
	â€¢	Trained and evaluated on CIFAR-10.
	â€¢	Visualized attention maps across layers/heads to analyze spatial/contextual understanding.
	â€¢	Experiments with hyperparameters (patch size, layers, heads).

ğŸ““ Notebook: Assignment3.ipynb

â¸»

Assignment 4 â€“ Autoencoders & Variational Autoencoders (VAE)
	â€¢	Autoencoders on Fashion-MNIST:
	â€¢	Latent dimensions: 16 and 48.
	â€¢	Compared reconstruction errors and visual quality.
	â€¢	Performed latent space interpolation to study smooth transitions between samples.
	â€¢	Variational Autoencoder (VAE):
	â€¢	12-dimensional latent space.
	â€¢	Generated new Fashion-MNIST-like samples.
	â€¢	Analyzed diversity & quality of generated outputs.
	â€¢	Explored latent variable manipulation for controlled generation.

ğŸ““ Notebook: Assignment4.ipynb

â¸»

ğŸ› ï¸ Technologies Used
	â€¢	Python, NumPy, Matplotlib (core implementations from scratch)
	â€¢	PyTorch (for tensor operations & autograd)
	â€¢	Jupyter Notebook (experiments, analysis, and visualization)

â¸»

ğŸš€ How to Run
	1.	Clone this repository:
