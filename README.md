Deep Learning Assignments (IIT Hyderabad ‚Äì AI2100/AI5100/EE6380)

This repository contains my solutions to the Deep Learning course assignments at IIT Hyderabad (Spring 2025).
Each assignment is implemented in a separate Jupyter Notebook (.ipynb) and covers key deep learning concepts ‚Äî from foundational algorithms to modern architectures like Transformers and VAEs.

‚∏ª

üìÇ Repository Structure
	‚Ä¢	Assignment 1 ‚Äì Perceptron, Gradient Descent, and MLP
	‚Ä¢	Assignment 2 ‚Äì 3D CNNs & Sequence Models (Elman, LSTM, GRU)
	‚Ä¢	Assignment 3 ‚Äì Word2Vec & Vision Transformer (ViT)
	‚Ä¢	Assignment 4 ‚Äì Autoencoders & Variational Autoencoders (VAE)

‚∏ª

üìò Assignment Details

Assignment 1 ‚Äì Fundamentals of Deep Learning
	‚Ä¢	Implemented Perceptron Learning Algorithm on linearly separable and noisy datasets.
	‚Ä¢	Implemented Gradient Descent for binary classification, comparing with Perceptron.
	‚Ä¢	Built an MLP (Multi-layer Perceptron) with a single hidden layer from scratch, including:
	‚Ä¢	Forward pass
	‚Ä¢	Backpropagation
	‚Ä¢	Gradient updates
	‚Ä¢	Experiments with weight initialization, learning rates, loss functions, and hidden layer size.

üìì Notebook: Assignment1.ipynb

‚∏ª

Assignment 2 ‚Äì Convolutional Neural Networks & Sequence Models
	‚Ä¢	Implemented a 3D Convolutional Neural Network (3D CNN) for MNIST point-cloud data:
	‚Ä¢	3D Convolution
	‚Ä¢	3D Pooling (Max, Avg, Global Avg)
	‚Ä¢	MLP classifier
	‚Ä¢	Complete 3D CNN pipeline with classification
	‚Ä¢	Designed a dataset for Balanced Parentheses Counting and trained sequence models:
	‚Ä¢	Elman RNN
	‚Ä¢	LSTM
	‚Ä¢	GRU
	‚Ä¢	Compared learning curves and performance against a simple baseline.

üìì Notebook: Assignment2.ipynb

‚∏ª

Assignment 3 ‚Äì Word2Vec & Vision Transformer
	‚Ä¢	Word2Vec (Skip-gram with Negative Sampling):
	‚Ä¢	Preprocessed text8 dataset (tokenization, vocabulary creation).
	‚Ä¢	Implemented Skip-gram with negative sampling from scratch.
	‚Ä¢	Trained embeddings and evaluated via:
	‚Ä¢	Visualization (SVD projections)
	‚Ä¢	Word similarity tasks (e.g., analogies like king - man + woman ‚âà queen).
	‚Ä¢	Analyzed impact of hyperparameters.
	‚Ä¢	Vision Transformer (ViT) on CIFAR-10:
	‚Ä¢	Implemented an Encoder-only ViT: patch embeddings, positional encodings, Transformer encoder, CLS token.
	‚Ä¢	Trained and evaluated on CIFAR-10.
	‚Ä¢	Visualized attention maps across layers/heads to analyze spatial/contextual understanding.
	‚Ä¢	Experiments with hyperparameters (patch size, layers, heads).

üìì Notebook: Assignment3.ipynb

‚∏ª

Assignment 4 ‚Äì Autoencoders & Variational Autoencoders (VAE)
	‚Ä¢	Autoencoders on Fashion-MNIST:
	‚Ä¢	Latent dimensions: 16 and 48.
	‚Ä¢	Compared reconstruction errors and visual quality.
	‚Ä¢	Performed latent space interpolation to study smooth transitions between samples.
	‚Ä¢	Variational Autoencoder (VAE):
	‚Ä¢	12-dimensional latent space.
	‚Ä¢	Generated new Fashion-MNIST-like samples.
	‚Ä¢	Analyzed diversity & quality of generated outputs.
	‚Ä¢	Explored latent variable manipulation for controlled generation.

üìì Notebook: Assignment4.ipynb

‚∏ª

üõ†Ô∏è Technologies Used
	‚Ä¢	Python, NumPy, Matplotlib (core implementations from scratch)
	‚Ä¢	PyTorch (for tensor operations & autograd)
	‚Ä¢	Jupyter Notebook (experiments, analysis, and visualization)

‚∏ª

